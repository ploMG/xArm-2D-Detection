# xArm Object Retrieval Routines

##  General Info

This project utilizes a microphone and Vosk speech recognition to receive a user's voice command, which will activate an overhead camera and a lateral camera and run YOLOv5 object detection to locate an object's position. An xArm 5 Lite with a gripper attachment will then execute a retrieval routine and return to its initial position to repeat the process.

<p align="center">
  <img src="https://github.com/ploMG/xArm-2D-Detection/assets/128610413/16b094ce-6d5e-4348-8bea-8a8873521560">
</p>
  
There are two routines:

* Manual mode - the user says the name of an object, and the arm will fetch the first instance it can detect. If the object is not detected, the arm will wait idly until an instance is introduced into the environment.

* Automatic mode - the user says "automatic," and the arm will dispose of all instances of trash. If there are no more instances detected, the arm will return to its initial position.

*Note: Parameters for the arm, such as movement speed and gripper strength, can be adjusted as necessary. Check [xArm API](https://github.com/xArm-Developer/xArm-Python-SDK/blob/master/doc/api/xarm_api.md) for more info.*


## Setup

Directory paths should be set within main.py and detect.py. Explanations for each component of the detection environment can be found below:

### Vosk

Vosk speech recognition models can be downloaded from the [official page](https://alphacephei.com/vosk/models). This project uses the small US English model.

When main.py is run, an audio stream begins that will continuously transcribe the user's input until a keyword to activate a retrieval routine is detected.

### Camera

The cameras' coordinates are not inherently synced to the xArm's, so functions from the OpenCV library are used to preprocess the captures prior to running YOLOv5. Calibration is done by taking images of the arm at each of the four corners of the defined environment and finding the pixel coordinates.

<p align="center">
  <img src="https://github.com/ploMG/xArm-2D-Detection/assets/128610413/5386357c-7dd7-473e-b57c-923e3d5b86e4">
</p>

### YOLOv5

After downloading [YOLOv5](https://github.com/ultralytics/yolov5), the default detect.py should be replaced by the modified detect.py found in the src folder. Images for a custom dataset can be gathered using the create\_training\_data.py script. The YOLOv5 train.py script can then train the model by running the following command:

```bash
python train.py --data bottle_det.yaml --weights yolov5s.pt --cfg yolov5s.yaml --batch 32 --epochs 100 --name bottle_det
```

This project uses a dataset that differentiates between bottles and crushed bottles (trash). The testing, training, and validation images and labels are found in the data folder. Autogenerated result figures from running train.py with this data is available in the doc folder.

After detecting an object, the generated bounding box coordinates will be used to calculate the arm's X,Y, and Z position, as well as the width of the object for the gripper.

<p align="center">
  <img src="https://github.com/ploMG/xArm-2D-Detection/assets/128610413/61819c1b-db9c-4f9b-8c8b-f80ab79a3dde">
</p>


## Samples

https://github.com/ploMG/xArm-2D-Detection/assets/128610413/a0752bbd-86e3-4ae1-9345-518af98d8ec0

> Automatic mode demonstration

https://github.com/ploMG/xArm-2D-Detection/assets/128610413/8a1496c2-414e-479d-a269-641ea5f1d9dd

> Manual mode demonstration

